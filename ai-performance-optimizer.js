#!/usr/bin/env node

/**
 * AI Performance Optimizer
 * Comprehensive optimization for AI model performance
 */

import axios from 'axios';

const OLLAMA_URL = 'http://localhost:11434';
const BACKEND_URL = 'http://localhost:3001';

const models = [
    { name: 'phi3:mini', purpose: 'Fast sentiment analysis', priority: 1 },
    { name: 'llama3.1:8b', purpose: 'Technical analysis', priority: 2 },
    { name: 'mistral:7b', purpose: 'Risk assessment', priority: 3 },
    { name: 'codellama:13b', purpose: 'Strategy analysis', priority: 4 }
];

async function checkOllamaStatus() {
    try {
        const response = await axios.get(`${OLLAMA_URL}/api/ps`);
        return response.data.models || [];
    } catch (error) {
        console.error('âŒ Ollama is not running');
        return null;
    }
}

async function preloadModel(modelName, keepAlive = '60m') {
    console.log(`ðŸ”¥ Preloading ${modelName}...`);

    try {
        const startTime = Date.now();

        // Send a simple prompt to load the model
        const response = await axios.post(`${OLLAMA_URL}/api/generate`, {
            model: modelName,
            prompt: 'Ready',
            stream: false,
            keep_alive: keepAlive,
            options: {
                temperature: 0.1,
                max_tokens: 5
            }
        }, { timeout: 120000 }); // 2 minute timeout for loading

        const loadTime = Date.now() - startTime;
        console.log(`âœ… ${modelName} loaded in ${loadTime}ms`);
        return true;
    } catch (error) {
        console.log(`âŒ Failed to load ${modelName}: ${error.message}`);
        return false;
    }
}

async function testModelResponse(modelName) {
    console.log(`ðŸ§ª Testing ${modelName} response time...`);

    try {
        const startTime = Date.now();

        const response = await axios.post(`${OLLAMA_URL}/api/generate`, {
            model: modelName,
            prompt: 'AAPL stock analysis: BUY/SELL/HOLD?',
            stream: false,
            options: {
                temperature: 0.1,
                max_tokens: 10
            }
        }, { timeout: 30000 });

        const responseTime = Date.now() - startTime;
        const aiResponse = response.data.response || 'No response';

        console.log(`âœ… ${modelName}: ${responseTime}ms - "${aiResponse.substring(0, 50)}..."`);
        return { success: true, time: responseTime, response: aiResponse };
    } catch (error) {
        console.log(`âŒ ${modelName}: Failed - ${error.message}`);
        return { success: false, time: 0, response: null };
    }
}

async function optimizeOllamaSettings() {
    console.log('ðŸ”§ Optimizing Ollama settings...');

    // Check current settings
    try {
        const response = await axios.get(`${OLLAMA_URL}/api/version`);
        console.log(`âœ… Ollama version: ${response.data.version}`);
    } catch (error) {
        console.log('âŒ Cannot connect to Ollama');
        return false;
    }

    return true;
}

async function testBackendPerformance() {
    console.log('ðŸ§ª Testing backend AI ensemble performance...');

    try {
        const startTime = Date.now();

        const response = await axios.post(`${BACKEND_URL}/api/ai/ensemble`, {
            symbol: 'AAPL',
            portfolio_value: 100000
        }, {
            headers: { 'Content-Type': 'application/json' },
            timeout: 120000 // 2 minutes
        });

        const responseTime = Date.now() - startTime;
        const data = response.data;

        console.log(`âœ… Backend ensemble: ${responseTime}ms`);
        console.log(`   Recommendation: ${data.recommendation} (${Math.round(data.confidence * 100)}%)`);
        console.log(`   Models responded: ${data.ensemble.models_responded}/${data.ensemble.models_total}`);

        // Check which models are working
        Object.entries(data.analyses).forEach(([type, analysis]) => {
            const isWorking = !analysis.analysis.includes('Fallback:');
            console.log(`   ${type}: ${isWorking ? 'âœ…' : 'âŒ'} ${isWorking ? 'AI' : 'Fallback'}`);
        });

        return data;
    } catch (error) {
        console.log(`âŒ Backend test failed: ${error.message}`);
        return null;
    }
}

async function main() {
    console.log('ðŸš€ AI Performance Optimizer Starting...\n');

    // Step 1: Check Ollama status
    console.log('ðŸ“Š Step 1: Checking Ollama status...');
    const loadedModels = await checkOllamaStatus();
    if (!loadedModels) return;

    console.log(`Currently loaded models: ${loadedModels.length}`);
    loadedModels.forEach(model => {
        console.log(`  - ${model.name}: ${(model.size / 1024 / 1024 / 1024).toFixed(1)}GB`);
    });

    // Step 2: Optimize Ollama settings
    console.log('\nðŸ”§ Step 2: Optimizing Ollama settings...');
    await optimizeOllamaSettings();

    // Step 3: Preload all models
    console.log('\nðŸ”¥ Step 3: Preloading all models...');
    for (const model of models) {
        await preloadModel(model.name);
        await new Promise(resolve => setTimeout(resolve, 2000)); // Wait 2s between models
    }

    // Step 4: Test individual model responses
    console.log('\nðŸ§ª Step 4: Testing individual model responses...');
    const testResults = {};
    for (const model of models) {
        testResults[model.name] = await testModelResponse(model.name);
        await new Promise(resolve => setTimeout(resolve, 1000)); // Wait 1s between tests
    }

    // Step 5: Test backend ensemble
    console.log('\nðŸŽ¯ Step 5: Testing backend ensemble performance...');
    const ensembleResult = await testBackendPerformance();

    // Step 6: Generate report
    console.log('\nðŸ“Š Performance Report:');
    console.log('='.repeat(50));

    let workingModels = 0;
    models.forEach(model => {
        const result = testResults[model.name];
        if (result.success) {
            workingModels++;
            console.log(`âœ… ${model.name}: ${result.time}ms - ${model.purpose}`);
        } else {
            console.log(`âŒ ${model.name}: Failed - ${model.purpose}`);
        }
    });

    console.log(`\nðŸ“ˆ Summary:`);
    console.log(`   Working models: ${workingModels}/${models.length} (${Math.round(workingModels / models.length * 100)}%)`);

    if (ensembleResult) {
        const realAI = Object.values(ensembleResult.analyses).filter(a => !a.analysis.includes('Fallback:')).length;
        console.log(`   Real AI responses: ${realAI}/4 (${Math.round(realAI / 4 * 100)}%)`);
        console.log(`   Backend response time: ${ensembleResult.performance.response_time_ms}ms`);
    }

    // Recommendations
    console.log(`\nðŸ’¡ Recommendations:`);
    if (workingModels < models.length) {
        console.log(`   - Increase timeout settings for failing models`);
        console.log(`   - Consider using smaller/faster models for production`);
        console.log(`   - Ensure sufficient RAM (recommend 16GB+ for all 4 models)`);
    }
    if (ensembleResult && ensembleResult.performance.response_time_ms > 10000) {
        console.log(`   - Response time is high - consider parallel processing`);
        console.log(`   - Implement better caching mechanisms`);
    }

    console.log(`\nâœ… Optimization complete!`);
}

main().catch(console.error);
